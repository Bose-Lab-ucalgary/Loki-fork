{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c44d2c-2bef-4280-96fc-085fc72167da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b87fc5-3b09-4637-99a4-dc72d9c2abcd",
   "metadata": {},
   "source": [
    "## Load OmiCLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ec8bb-ad59-4a3a-b508-1b330e5d8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/path/to/chekpoint'\n",
    "model, preprocess = create_model_from_pretrained(\"coca_ViT-L-14\", device = DEVICE, pretrained=model_path)\n",
    "tokenizer = get_tokenizer('coca_ViT-L-14')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689c881-1ea6-4b08-8b07-5c5d55b635cb",
   "metadata": {},
   "source": [
    "## Encode Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2f23-85d8-4bf6-b7f3-6dcd7fcadb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/path/to/image'\n",
    "image = Image.open(image_path)\n",
    "image.resize((20, 20))\n",
    "image_input = torch.stack([preprocess(image)])\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f186813-515f-4528-96ed-e9d8abe50986",
   "metadata": {},
   "source": [
    "## Encode Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5112b-32ab-45af-99ba-6999fa1ef93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['gene_list1',\n",
    "             'gene_list2']\n",
    "text_input = tokenizer(text_list)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38395db-0e3d-4fc2-a416-942653f5d8f5",
   "metadata": {},
   "source": [
    "## Retrieve text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e099ad-3107-4fe1-a45d-42fb8175fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_similarity = image_features.norm(dim=-1) @ text_features.norm(dim=-1).T\n",
    "print(\"Image to text retrieval based on cosine similarity with the image:\")\n",
    "similarity_scores, ranked_idx = torch.sort(dot_similarity, descending=True)\n",
    "for idx, score in zip(ranked_idx, similarity_scores):\n",
    "    print(f\"\\\"{text_list[idx]}\\\": {score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-kernel",
   "language": "python",
   "name": "clip-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
